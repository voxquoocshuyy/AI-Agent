# Task T1.2: Document Processing & Database Infrastructure
**Status:** New
**Sprint:** 1
**Priority:** High
**Assigned To:** TBD

## Description
Phát triển hệ thống xử lý tài liệu (PDF, DOCX, CSV) và chuyển đổi thành vector embeddings để lưu trữ trong vector database, đồng thời thiết lập cơ sở dữ liệu và cơ sở hạ tầng.

## Technical Details

### 1. Document Processing Architecture
```
AI.Agent/
└── src/
    └── AI.Agent.Infrastructure/
        └── DocumentProcessing/
            ├── Interfaces/
            │   ├── IDocumentProcessor.cs
            │   └── IDocumentExtractor.cs
            ├── Extractors/
            │   ├── PdfExtractor.cs
            │   ├── DocxExtractor.cs
            │   └── CsvExtractor.cs
            ├── Processors/
            │   ├── TextProcessor.cs
            │   └── ChunkProcessor.cs
            └── Models/
                ├── Document.cs
                └── DocumentChunk.cs
```

### 2. Database Infrastructure
```
AI.Agent/
└── src/
    └── AI.Agent.Infrastructure/
        ├── Persistence/
        │   ├── Configurations/
        │   ├── Repositories/
        │   └── Context/
        └── Logging/
            ├── Serilog/
            └── Elasticsearch/
```

### 3. Core Implementation
```csharp
// Document Processing Interface
public interface IDocumentProcessor
{
    Task<Document> ProcessAsync(Stream documentStream, string fileName);
    Task<IEnumerable<DocumentChunk>> ChunkAsync(Document document);
}

// Document Model
public class Document
{
    public string Id { get; set; }
    public string FileName { get; set; }
    public string Content { get; set; }
    public DocumentType Type { get; set; }
    public Dictionary<string, string> Metadata { get; set; }
}

// Using Microsoft.Extensions.AI for embeddings
public class DocumentEmbeddingService
{
    private readonly IEmbeddingGenerator _embeddingGenerator;
    
    public async Task<float[]> GenerateEmbeddingAsync(string text)
    {
        var embedding = await _embeddingGenerator.GenerateEmbeddingAsync(text);
        return embedding.Vector;
    }
}
```

### 4. Document Processing Pipeline
1. **Document Extraction**
   - PDF: Using iTextSharp for text extraction
   - DOCX: Using DocumentFormat.OpenXml
   - CSV: Using CsvHelper with custom mapping

2. **Text Processing**
   - Text cleaning and normalization
   - Language detection
   - Special character handling
   - Metadata extraction

3. **Chunking Strategy**
   - Semantic chunking based on paragraphs
   - Overlap handling between chunks
   - Size optimization for embedding generation

4. **Vector Generation**
   - Using Azure OpenAI embeddings
   - Batch processing for efficiency
   - Error handling and retry logic

## Subtasks

### 1. Database Infrastructure [Estimation: 4 hours]
- [x] Create database schema
- [x] Configure Entity Framework Core
- [x] Set up migrations
- [x] Configure connection strings
- [x] Add database health check

### 2. Repository Pattern [Estimation: 3 hours]
- [x] Create base repository interface
- [x] Implement generic repository
- [x] Create document repository
- [x] Set up unit of work pattern
- [x] Add repository tests

### 3. Document Extractors [Estimation: 6 hours]
- [ ] Implement PDF extractor
- [ ] Implement DOCX extractor
- [ ] Implement CSV extractor
- [ ] Add unit tests for extractors

### 4. Text Processing [Estimation: 4 hours]
- [ ] Implement text cleaning
- [ ] Add language detection
- [ ] Implement metadata extraction
- [ ] Add unit tests

### 5. Chunking System [Estimation: 4 hours]
- [ ] Implement semantic chunking
- [ ] Add overlap handling
- [ ] Optimize chunk sizes
- [ ] Add unit tests

### 6. Vector Generation [Estimation: 4 hours]
- [ ] Implement embedding generation
- [ ] Add batch processing
- [ ] Implement error handling
- [ ] Add integration tests

### 7. Docker & Monitoring [Estimation: 3 hours]
- [x] Create Dockerfile for API
- [x] Set up docker-compose.yml
- [x] Configure PostgreSQL container
- [x] Set up Elasticsearch container
- [x] Configure Kibana container
- [x] Set up Serilog
- [x] Configure Elasticsearch sink
- [x] Create logging middleware
- [x] Set up health checks
- [x] Configure monitoring

## Dependencies
- T1.1 (Project Setup)
- PostgreSQL
- Entity Framework Core
- Docker
- Serilog
- Elasticsearch
- Kibana
- Azure OpenAI Service access
- Required NuGet packages

## Acceptance Criteria
- [x] Database is properly configured and accessible
- [x] Repository pattern is implemented correctly
- [x] Docker containers are running properly
- [x] Logging is working and sending data to Elasticsearch
- [x] Health checks are implemented and working
- [ ] Successfully extracts text from all supported formats
- [ ] Generates semantically meaningful chunks
- [ ] Produces valid vector embeddings
- [ ] Handles errors gracefully
- [ ] Unit tests pass with >90% coverage
- [ ] Integration tests pass

## Notes
- Use connection pooling for database
- Implement proper error handling
- Set up proper logging patterns
- Configure proper security for containers
- Document all configuration steps
- Implement proper logging for debugging
- Consider memory usage for large documents
- Add progress tracking for long-running operations
- Implement caching for frequently accessed documents

## Updated Estimation
- Original: 4 days
- New: 3.5 days (28 hours)
  - Database Infrastructure: 4 hours
  - Repository Pattern: 3 hours
  - Document Extractors: 6 hours
  - Text Processing: 4 hours
  - Chunking System: 4 hours
  - Vector Generation: 4 hours
  - Docker & Monitoring: 3 hours 